import { EvaluationDimension, TaskAnalysis, TimeContext } from './types.js';

/**
 * Quality Evaluation Dimension Generator
 * Supports configurable dimension count and expected score, each dimension can be scored 0-10 with any number, final score is average
 * Note: 3-tier guidelines (6, 8, 10 points) are for guidance only, actual scoring can be any number 0-10
 */
export class QualityDimensionGenerator {
	private dimensionCount: number;
	private expectedScore: number;
	
	constructor(config: { dimensionCount: number; expectedScore: number } = { dimensionCount: 5, expectedScore: 8 }) {
		this.dimensionCount = config.dimensionCount;
		this.expectedScore = config.expectedScore;
	}
	
	/**
	 * Generate dimension scoring prompt, supports configurable dimension count and expected score
	 */
	public async generateDimensionsPrompt(
		task: TaskAnalysis,
		timeContext: TimeContext,
		projectPath?: string,
		targetScore?: number
	): Promise<string> {
		// Use instance configuration or provided target score
		const dimensionCount = this.dimensionCount;
		const expectedScore = targetScore ?? this.expectedScore;

		return `🎯 **Important Note: Please strictly follow the format to ensure proper parsing and saving by subsequent programs!**

Based on the following task information, generate complete scoring standards for ${dimensionCount} evaluation dimensions:

## 📋 Task Information
- **Core Task**: ${task.coreTask}
- **Task Type**: ${task.taskType}
- **Complexity**: ${task.complexity}/5
- **Domain**: ${task.domain}
- **Key Elements**: ${task.keyElements.join(', ')}
- **Objectives**: ${task.objectives.join(', ')}

## ⏰ Time Context
- **Current Time**: ${timeContext.formattedTime}
- **Year**: ${timeContext.year}, Month: ${timeContext.month}

## 🎯 Quality Target
- **Expected Score**: ${expectedScore}/10 points
- **Scoring Strictness**: ${expectedScore >= 8 ? 'High Standard (Strict Evaluation)' : expectedScore >= 6 ? 'Medium Standard (Moderate Evaluation)' : 'Basic Standard (Lenient Evaluation)'}
- **Quality Requirements**: ${expectedScore >= 8 ? 'Pursue excellence, perfect details' : expectedScore >= 6 ? 'Good quality, main functions complete' : 'Basically usable, meet basic needs'}

---

## ⚡ **Format Requirements (Please strictly follow)**

**Must output in the following precise format, each dimension includes title, description, importance, and scoring guidance:**

### Dimension 1: [Specific Dimension Name] (0-10 points)
**Description**: [Concise and clear dimension explanation, summarize evaluation scope in one sentence]
**Importance**: [One sentence explaining why this dimension is important]
**Scoring Guidance**:
- **10 points**: [Specific and clear excellence standard, actionable and measurable]
- **8 points**: [Specific and clear good standard, actionable and measurable]
- **6 points**: [Specific and clear passing standard, actionable and measurable]

### Dimension 2: [Specific Dimension Name] (0-10 points)
**Description**: [Concise and clear dimension explanation, summarize evaluation scope in one sentence]
**Importance**: [One sentence explaining why this dimension is important]
**Scoring Guidance**:
- **10 points**: [Specific and clear excellence standard, actionable and measurable]
- **8 points**: [Specific and clear good standard, actionable and measurable]
- **6 points**: [Specific and clear passing standard, actionable and measurable]

**[Continue generating ${dimensionCount} dimensions, strictly following the above format]**

---

## ✅ **Quality Requirements**
1. **Consistent Format**: Each dimension must include title, description, importance, scoring guidance
2. **Specific Standards**: Each scoring guidance should be specific and actionable, avoid vague expressions
3. **Comprehensive Coverage**: ${dimensionCount} dimensions should cover all key aspects of the task
4. **Professional**: Meet professional standards in the ${task.domain} domain
5. **Target Matching**: Scoring standard strictness matches expected score ${expectedScore}/10 points

## ⚠️ **Key Notes**
- **Strict Format**: Must use "### Dimension X:" as beginning, "**Description**:", "**Importance**:", "**Scoring Guidance**:" labels
- **Complete Output**: Must generate complete ${dimensionCount} dimensions, cannot omit any
- **Clear Standards**: 10, 8, 6 point standards should have obvious differences for easy scoring
- **Strong Practicality**: Generated standards should be convenient for actual use and scoring

🚀 **Please now start generating ${dimensionCount} evaluation dimensions!**`;
	}

	/**
	 * Create dimension template, supports configurable count
	 */
	public createDimensions(dimensionCount: number = 5): EvaluationDimension[] {
		return Array.from({ length: dimensionCount }, (_, index) => ({
			name: `Dimension ${index + 1}`,
			weight: 1 / dimensionCount, // Dynamically calculate weight
			description: 'Detailed description to be generated by LLM',
			criteria: [
				'10 points: Excellent performance',
				'8 points: Good performance',
				'6 points: Passing performance'
			],
			evaluationMethod: '0-10 point scoring (can be any number)',
			importance: 'Importance explanation to be generated by LLM'
		}));
	}

	/**
	 * Parse LLM-generated dimension scoring text (3-tier guidance)
	 */
	public parseDimensionsFromText(dimensionsText: string, dimensionCount: number = 5): EvaluationDimension[] {
		const dimensions: EvaluationDimension[] = [];
		
		// Split text by dimensions
		const dimensionBlocks = dimensionsText.split(/### Dimension \d+：/).slice(1);
		if (dimensionBlocks.length === 0) {
			// Try alternative patterns
			const altBlocks = dimensionsText.split(/### \d+\./).slice(1);
			dimensionBlocks.push(...altBlocks);
		}

		for (let i = 0; i < Math.min(dimensionBlocks.length, dimensionCount); i++) {
			const block = dimensionBlocks[i].trim();
			const lines = block.split('\n').map(line => line.trim()).filter(line => line);
			
			// Parse dimension information
			const dimension: EvaluationDimension = {
				name: '',
				weight: 1 / dimensionCount,
				description: '',
				criteria: [],
				evaluationMethod: '0-10 point scoring (can be any number)',
				importance: ''
			};

			// Parse dimension name
			const firstLine = lines[0] || '';
			const nameMatch = firstLine.match(/^(.+?)\s*\(0-10\s*points?\)/);
			if (nameMatch) {
				dimension.name = nameMatch[1].trim();
			}

			// Parse description and importance
			for (const line of lines) {
				if (line.startsWith('**Description**:')) {
					dimension.description = line.substring(16).trim();
				} else if (line.startsWith('**Importance**:')) {
					dimension.importance = line.substring(15).trim();
				}
			}

			// Parse scoring guidance (3 tiers: 6, 8, 10 points)
			const criteria: string[] = [];
			const tenPointMatch = block.match(/- \*\*10 points?\*\*[：:](.+?)$/m);
			const eightPointMatch = block.match(/- \*\*8 points?\*\*[：:](.+?)$/m);
			const sixPointMatch = block.match(/- \*\*6 points?\*\*[：:](.+?)$/m);

			if (tenPointMatch) criteria.push(`10 points: ${tenPointMatch[1].trim()}`);
			if (eightPointMatch) criteria.push(`8 points: ${eightPointMatch[1].trim()}`);
			if (sixPointMatch) criteria.push(`6 points: ${sixPointMatch[1].trim()}`);

			dimension.criteria = criteria.length > 0 ? criteria : [
				'10 points: Excellent performance',
				'8 points: Good performance',
				'6 points: Passing performance'
			];

			// Set default name
			if (!dimension.name) {
				dimension.name = `Dimension ${i + 1}`;
			}

			dimensions.push(dimension);
		}

		// Ensure specified number of dimensions
		while (dimensions.length < dimensionCount) {
			const defaultDimension = this.createDimensions(1)[0];
			defaultDimension.name = `Dimension ${dimensions.length + 1}`;
			defaultDimension.weight = 1 / dimensionCount;
			dimensions.push(defaultDimension);
		}

		return dimensions.slice(0, dimensionCount);
	}

	/**
	 * Generate scoring table (supports configurable dimension count)
	 */
	public generateScoringTable(dimensions: EvaluationDimension[]): string {
		const dimensionCount = dimensions.length;
		let table = `# Scoring Standards Table (Total 10 points, ${dimensionCount} dimensions with equal weight)\n\n`;
		
		dimensions.forEach((dimension, dimIndex) => {
			table += `## ${dimIndex + 1}. ${dimension.name} (0-10 points)\n`;
			table += `**Description**: ${dimension.description}\n`;
			table += `**Scoring Guidance**:\n`;
			dimension.criteria.forEach((criterion: string) => {
				table += `- ${criterion}\n`;
			});
			table += `**Score**: __/10 points (can be any number, like 7.5 points)\n\n`;
		});

		table += `## Total Score Calculation\n`;
		table += `| Dimension | Score | Note |\n`;
		table += `|-----------|-------|------|\n`;
		dimensions.forEach((dimension, index) => {
			table += `| ${dimension.name} | __/10 points | Can be any number |\n`;
		});
		table += `| **Average Score** | **__/10 points** | **Average of all dimensions** |\n`;

		return table;
	}
}